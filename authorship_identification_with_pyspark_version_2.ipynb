{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "541ee0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os \n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"AuthorshipIdentificationWithPyspark\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1g\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04c4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "372d1d77",
   "metadata": {},
   "source": [
    "##### Lines for each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd223c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lafille2.txt 645\n",
      "elixir2.txt 203\n",
      "chef2.txt 334\n",
      "chabert3.txt 883\n",
      "uncoeur3.txt 503\n",
      "bovary3.txt 6047\n",
      "bouvard2.txt 6569\n",
      "educati1.txt 10149\n",
      "salammb1.txt 4109\n"
     ]
    }
   ],
   "source": [
    "BASH_PATH = './data/'\n",
    "\n",
    "folders = [i for i in os.listdir(BASH_PATH) if not i.startswith('.') ] \n",
    "authors, texts, books = [], [], []\n",
    "for folder in folders :\n",
    "    files = [f for f in os.listdir(BASH_PATH+folder) if not f.startswith(\".\")]\n",
    "    for file in files:\n",
    "        with open(BASH_PATH+folder+\"/\"+file, 'r') as io_file :\n",
    "            lines = io_file.readlines()\n",
    "            print(file, len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d540fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip install pandas \n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22eeb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877a5e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "def create_dataset():\n",
    "    # base path for data \n",
    "    BASH_PATH = './data/'\n",
    "\n",
    "    folders = [i for i in os.listdir(BASH_PATH) if not i.startswith('.') ] \n",
    "    authors, texts, books = [], [], []\n",
    "    for folder in folders :\n",
    "        files = [f for f in os.listdir(BASH_PATH+folder) if not f.startswith(\".\")]\n",
    "        for file in files:\n",
    "            with open(BASH_PATH+folder+\"/\"+file, 'r') as io_file :\n",
    "                lines = io_file.readlines()\n",
    "                i=0\n",
    "                while i<len(lines) :\n",
    "                    if i+50> len(lines):\n",
    "                        text = lines[i:]\n",
    "                        break\n",
    "                    text = \"\\n\".join(lines[i: i+50])\n",
    "                    i+=50\n",
    "                    text = re.sub('\\W+|_', ' ', text)\n",
    "                    texts.append(text)\n",
    "                    books.append(file.split(\".\")[0])\n",
    "                    authors.append(folder)\n",
    "                      \n",
    "                \n",
    "    pd.DataFrame({\"text\":texts, \"author\":authors, \"book\":books}).to_csv(\"data.csv\", index=False)\n",
    "                        \n",
    "# create dataset\n",
    "create_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32a3956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+\n",
      "|                text|author|    book|\n",
      "+--------------------+------+--------+\n",
      "|CHAPITRE I PHYSIO...|Balzec|lafille2|\n",
      "| Je m étonne mon ...|Balzec|lafille2|\n",
      "| Ah dit le facteu...|Balzec|lafille2|\n",
      "| Drôle répondit H...|Balzec|lafille2|\n",
      "| Pantoufle il y e...|Balzec|lafille2|\n",
      "+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"data.csv\")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a080b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'author', 'book']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c169ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"text\", \"author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5544fb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|author|\n",
      "+--------------------+------+\n",
      "|CHAPITRE I PHYSIO...|Balzec|\n",
      "| Je m étonne mon ...|Balzec|\n",
      "| Ah dit le facteu...|Balzec|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f18bbcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  author|count|\n",
      "+--------+-----+\n",
      "|  Balzec|   39|\n",
      "|Flaubert|  545|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"author\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e75915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35c98d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, IDF, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49425bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"mytokens\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"mytokens\", outputCol=\"filtered_tokens\")\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"vectorizedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33eb9df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labelEncoder = StringIndexer(inputCol=\"author\", outputCol=\"label\").fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b1706df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labelEncoder.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aaaca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDF, testDF) = df.randomSplit((0.6, 0.3), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc6654c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+\n",
      "|                text|  author|label|\n",
      "+--------------------+--------+-----+\n",
      "| A bas les assomm...|Flaubert|  0.0|\n",
      "| A côté s écria m...|Flaubert|  0.0|\n",
      "| Adieu Elle ne lu...|Flaubert|  0.0|\n",
      "| Ah ah dit don Ju...|  Balzec|  1.0|\n",
      "| Ah c est trop fo...|Flaubert|  0.0|\n",
      "| Ah dit le facteu...|  Balzec|  1.0|\n",
      "| Ah la Révolution...|Flaubert|  0.0|\n",
      "| Ah qu un négocia...|Flaubert|  0.0|\n",
      "| Ah vous trouvez ...|Flaubert|  0.0|\n",
      "| Ainsi je puis di...|Flaubert|  0.0|\n",
      "| Allons bon dit H...|Flaubert|  0.0|\n",
      "| Allons une prise...|Flaubert|  0.0|\n",
      "| Alors je vais fa...|  Balzec|  1.0|\n",
      "| Alors tu comptes...|Flaubert|  0.0|\n",
      "| Approchez vous d...|Flaubert|  0.0|\n",
      "| Au bout de dix m...|Flaubert|  0.0|\n",
      "| Au moment de par...|Flaubert|  0.0|\n",
      "| Aux vôtres C éta...|Flaubert|  0.0|\n",
      "| Bah l avenir est...|Flaubert|  0.0|\n",
      "| Barca nous venon...|Flaubert|  0.0|\n",
      "+--------------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trainDF.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af13417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+\n",
      "|                text|  author|label|\n",
      "+--------------------+--------+-----+\n",
      "| Acide de sucre f...|Flaubert|  0.0|\n",
      "| Adieu dit Frédér...|Flaubert|  0.0|\n",
      "| Ah bah il en a d...|Flaubert|  0.0|\n",
      "| Ah c est vrai Et...|Flaubert|  0.0|\n",
      "| Ah c est vrai re...|Flaubert|  0.0|\n",
      "| Ah je n ai jamai...|Flaubert|  0.0|\n",
      "| Ah les affiches ...|Flaubert|  0.0|\n",
      "| Ah pardon monsie...|Flaubert|  0.0|\n",
      "| Ah pas encore re...|Flaubert|  0.0|\n",
      "| Ajoute que les v...|  Balzec|  1.0|\n",
      "| Allons mon bon a...|Flaubert|  0.0|\n",
      "| Allons à son ate...|  Balzec|  1.0|\n",
      "| Allévy transform...|Flaubert|  0.0|\n",
      "| Après trois heur...|Flaubert|  0.0|\n",
      "| Arrêtez s écria ...|Flaubert|  0.0|\n",
      "| Asseyez vous dit...|Flaubert|  0.0|\n",
      "| Au bout de la se...|Flaubert|  0.0|\n",
      "| Autre sujet d ét...|Flaubert|  0.0|\n",
      "| Aux approches du...|Flaubert|  0.0|\n",
      "| Bien bien monsie...|Flaubert|  0.0|\n",
      "+--------------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98bb406",
   "metadata": {},
   "source": [
    "### Building Logistic Regression Pipeline \n",
    "* tokenizer\n",
    "* stop words removing\n",
    "* vectorize words \n",
    "* IF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "012b7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "663ff819",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"vectorizedFeatures\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "321bf77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9760638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8ce9a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_09c79452f06a"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b00049a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='Pipeline_09c79452f06a', name='stages', doc='a list of pipeline stages')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd9e2035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/28 22:34:36 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/04/28 22:34:36 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/04/28 22:34:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/04/28 22:34:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bd86b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_56fbb19f9550"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fcb2d",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b927a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "947bfd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'author',\n",
       " 'label',\n",
       " 'mytokens',\n",
       " 'filtered_tokens',\n",
       " 'rawFeatures',\n",
       " 'vectorizedFeatures',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bddc0774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/28 22:34:47 WARN DAGScheduler: Broadcasting large task binary with size 1024.8 KiB\n",
      "+--------------------+--------+-----+--------------------+----------+\n",
      "|                text|  author|label|         probability|prediction|\n",
      "+--------------------+--------+-----+--------------------+----------+\n",
      "| Acide de sucre f...|Flaubert|  0.0|[0.99999999926659...|       0.0|\n",
      "| Adieu dit Frédér...|Flaubert|  0.0|[0.99999999999246...|       0.0|\n",
      "| Ah bah il en a d...|Flaubert|  0.0|[2.30360532082317...|       1.0|\n",
      "| Ah c est vrai Et...|Flaubert|  0.0|[0.99999999998176...|       0.0|\n",
      "| Ah c est vrai re...|Flaubert|  0.0|[0.99999999995752...|       0.0|\n",
      "| Ah je n ai jamai...|Flaubert|  0.0|[0.99999986978351...|       0.0|\n",
      "| Ah les affiches ...|Flaubert|  0.0|[0.99999999032948...|       0.0|\n",
      "| Ah pardon monsie...|Flaubert|  0.0|[0.99999787857289...|       0.0|\n",
      "| Ah pas encore re...|Flaubert|  0.0|[0.99999999916960...|       0.0|\n",
      "| Ajoute que les v...|  Balzec|  1.0|[9.10402763909350...|       1.0|\n",
      "| Allons mon bon a...|Flaubert|  0.0|[0.99999865744513...|       0.0|\n",
      "| Allons à son ate...|  Balzec|  1.0|[1.00472295501294...|       1.0|\n",
      "| Allévy transform...|Flaubert|  0.0|[0.99999999405892...|       0.0|\n",
      "| Après trois heur...|Flaubert|  0.0|[0.99999999379147...|       0.0|\n",
      "| Arrêtez s écria ...|Flaubert|  0.0|[0.99999999991122...|       0.0|\n",
      "| Asseyez vous dit...|Flaubert|  0.0|[0.99999998424737...|       0.0|\n",
      "| Au bout de la se...|Flaubert|  0.0|[0.25650170022468...|       1.0|\n",
      "| Autre sujet d ét...|Flaubert|  0.0|[0.99999999907453...|       0.0|\n",
      "| Aux approches du...|Flaubert|  0.0|[0.99999999893353...|       0.0|\n",
      "| Bien bien monsie...|Flaubert|  0.0|[0.99999988302878...|       0.0|\n",
      "+--------------------+--------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"text\", \"author\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a92d43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16b14a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba219331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/28 22:34:52 WARN DAGScheduler: Broadcasting large task binary with size 1032.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f951c938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9763313609467456"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a2932b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c4d8441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyspark/sql/context.py:159: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/28 22:35:00 WARN DAGScheduler: Broadcasting large task binary with size 1021.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_metric = MulticlassMetrics(predictions[\"label\", \"prediction\"].rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc816f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/28 22:35:02 WARN DAGScheduler: Broadcasting large task binary with size 1032.4 KiB\n",
      "Accuracy:  0.9763313609467456\n",
      "Precision:  1.0\n",
      "Recall:  0.6666666666666666\n",
      "F1Score:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "print(\"Accuracy: \", lr_metric.accuracy)\n",
    "# precision\n",
    "print(\"Precision: \", lr_metric.precision(1.0))\n",
    "# recall\n",
    "print(\"Recall: \", lr_metric.recall(1.0))\n",
    "# f1socre\n",
    "print(\"F1Score: \", lr_metric.fMeasure(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00c5a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/28 22:35:06 WARN DAGScheduler: Broadcasting large task binary with size 1016.5 KiB\n"
     ]
    }
   ],
   "source": [
    "y_true = predictions.select(\"label\")\n",
    "y_true = y_true.toPandas()\n",
    "y_pred = predictions.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78b1700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d8e37ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[157,   4],\n",
       "       [  0,   8]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a6089",
   "metadata": {},
   "source": [
    "### Create Pipeline For Naive Bayes Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fcc10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32ae1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=\"vectorizedFeatures\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f17b4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, idf, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8682a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb_model = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88b8c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb_model.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9e47724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/28 22:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1230.4 KiB\n",
      "+--------------------+--------+-----+--------------------+----------+\n",
      "|                text|  author|label|         probability|prediction|\n",
      "+--------------------+--------+-----+--------------------+----------+\n",
      "| Acide de sucre f...|Flaubert|  0.0|           [1.0,0.0]|       0.0|\n",
      "| Adieu dit Frédér...|Flaubert|  0.0|[1.0,8.0397233671...|       0.0|\n",
      "| Ah bah il en a d...|Flaubert|  0.0|[1.0,4.4884203975...|       0.0|\n",
      "| Ah c est vrai Et...|Flaubert|  0.0|[1.0,2.7080395496...|       0.0|\n",
      "| Ah c est vrai re...|Flaubert|  0.0|[1.0,5.7633008111...|       0.0|\n",
      "| Ah je n ai jamai...|Flaubert|  0.0|[1.0,1.8936641263...|       0.0|\n",
      "| Ah les affiches ...|Flaubert|  0.0|[1.0,3.2910625617...|       0.0|\n",
      "| Ah pardon monsie...|Flaubert|  0.0|[1.0,1.2074475252...|       0.0|\n",
      "| Ah pas encore re...|Flaubert|  0.0|[1.0,3.6320289396...|       0.0|\n",
      "| Ajoute que les v...|  Balzec|  1.0|           [0.0,1.0]|       1.0|\n",
      "| Allons mon bon a...|Flaubert|  0.0|[1.0,5.8140902160...|       0.0|\n",
      "| Allons à son ate...|  Balzec|  1.0|           [0.0,1.0]|       1.0|\n",
      "| Allévy transform...|Flaubert|  0.0|[1.0,1.8254506142...|       0.0|\n",
      "| Après trois heur...|Flaubert|  0.0|[1.0,3.8739787295...|       0.0|\n",
      "| Arrêtez s écria ...|Flaubert|  0.0|[1.0,2.1795722996...|       0.0|\n",
      "| Asseyez vous dit...|Flaubert|  0.0|[1.0,7.0028532177...|       0.0|\n",
      "| Au bout de la se...|Flaubert|  0.0|[1.0,6.4044157799...|       0.0|\n",
      "| Autre sujet d ét...|Flaubert|  0.0|[1.0,2.4372088277...|       0.0|\n",
      "| Aux approches du...|Flaubert|  0.0|           [1.0,0.0]|       0.0|\n",
      "| Bien bien monsie...|Flaubert|  0.0|[1.0,5.4427628765...|       0.0|\n",
      "+--------------------+--------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"text\", \"author\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3de00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b445be38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/28 22:40:48 WARN DAGScheduler: Broadcasting large task binary with size 1238.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9ddd670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
